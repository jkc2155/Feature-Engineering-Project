install.packages("ggplot2")
packs <- installed.packages() # Get the currently installed packages
exc <- names(packs[,’Package’])  # Get the names in a vector
av <- names(available.packages()[,1]) #Get names of available packages in Cran
ins <- av[!av %in% exc] #Make a list of all packages that you havent installed
i
install.packages("lime")
install.packages("googlesheets")
install.packages("bigrquery")
install.packages("jsonlite")
install.packages(c("e1071", "rpart", "igraph", "nnet", "randomForest", "caret", "kernlab", "glmnet", "ROCR", "gbm", "party", "arules", "tree", "klar", "rweka", "ipred", "lars", "earth", "corelearn", "mboost", "bigQueryR", "blscrapeR", "cdlTools", "dataRetrieval", "eechidna", "GetHFData", "googleAnalyticsR", "googleway", "gutenbergr", "ie2miscdata", "macleish", "muckrock", "nasadata", "oec", "osi", "pewdata", "TCGAretriever"))
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Select all numeric values
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
setwd("~/GitHub/Feature-Engineering-Project")
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Select all numeric values
df5 <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, CMA))
# Split data into training and test set at 3/4ths split
index <- sample(1:nrow(df5),round(0.75*nrow(df5)))
train <- df5[index,]
test <- df5[-index,]
###################################
## Fit a linear regression model ##
###################################
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$TMA)^2)/nrow(test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(train)
f <- as.formula(paste("TMA ~", paste(n[!n %in% "TMA"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
## Predicting TMA using the neural network ##
#############################################
View(test)
pr.nn <- compute(nn,test[,1:21])
save.image("~/GitHub/Feature-Engineering-Project/nn enviornment.RData")
test1 <- subset(test, select = -c(TMA))
pr.nn <- compute(nn,test1[,1:20])
test <- data.frame(test1, test$TMA)
View(test)
names(test)[names(test) == 'test.TMA'] <- 'TMA'
pr.nn <- compute(nn,test[,1:20])
pr.nn_ <- pr.nn$net.result*(max(f5$TMA)-min(df5$TMA))+min(df5$TMA)
pr.nn_ <- pr.nn$net.result*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
test.r <- (test$TMA)*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
print(paste(MSE.lm,MSE.nn))
par(mfrow=c(1,2))
plot(test$TMA,pr.nn,col='red',main='Real vs predicted NN',pch=18,cex=0.7)abline(0,1,lwd=2)legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$TMA,pr.nn,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$medv,pr.nn,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
outlier??
??outlier
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Select all numeric values
df5 <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, CMA))
outlierKD <- function(df5, var) {
TMA <- eval(substitute(var),eval(df5))
na1 <- sum(is.na(TMA))
m1 <- mean(TMA, na.rm = T)
par(mfrow=c(2, 2), oma=c(0,0,3,0))
boxplot(TMA, main="With outliers")
hist(TMA, main="With outliers", xlab=NA, ylab=NA)
outlier <- boxplot.stats(TMA)$out
mo <- mean(outlier)
TMA <- ifelse(TMA %in% outlier, NA, TMA)
boxplot(TMA, main="Without outliers")
hist(TMA, main="Without outliers", xlab=NA, ylab=NA)
title("Outlier Check", outer=TRUE)
na2 <- sum(is.na(TMA))
cat("Outliers identified:", na2 - na1, "n")
cat("Propotion (%) of outliers:", round((na2 - na1) / sum(!is.na(TMA))*100, 1), "n")
cat("Mean of the outliers:", round(mo, 2), "n")
m2 <- mean(TMA, na.rm = T)
cat("Mean without removing outliers:", round(m1, 2), "n")
cat("Mean if we remove outliers:", round(m2, 2), "n")
response <- readline(prompt="Do you want to remove outliers and to replace with NA? [yes/no]: ")
if(response == "y" | response == "yes"){
df5[as.character(substitute(var))] <- invisible(TMA)
assign(as.character(as.list(match.call())$df5), df5, envir = .GlobalEnv)
cat("Outliers successfully removed", "n")
return(invisible(df5))
} else{
cat("Nothing changed", "n")
return(invisible(TMA))
}
}
View(outlierKD)
df6 <- outlierKD(df5,TMA)
df6 <- data.frame(outlierKD(df5,TMA))
View(df6)
boxplot(df6$TMA)
boxplot(df5$TMA)
df5 <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, CMA))
boxplot(df5$TMA)
boxplot(df6$TMA)
outlierKD(df5,TMA))
outlierKD(df5,TMA)
# Split data into training and test set at 3/4ths split
index <- sample(1:nrow(df5),round(0.75*nrow(df5)))
train <- df5[index,]
test <- df5[-index,]
###################################
## Fit a linear regression model ##
###################################
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$TMA)^2)/nrow(test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(train)
f <- as.formula(paste("TMA ~", paste(n[!n %in% "TMA"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
## Predicting TMA using the neural network ##
#############################################
test1 <- subset(test, select = -c(TMA))
test <- data.frame(test1, test$TMA)
names(test)[names(test) == 'test.TMA'] <- 'TMA'
pr.nn <- compute(nn,test[,1:20])
pr.nn_ <- pr.nn$net.result*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
test.r <- (test$TMA)*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
##########################
## Compare the two MSEs ##
##########################
print(paste(MSE.lm,MSE.nn))
##########################
## Plot the performance ##
##########################
par(mfrow=c(1,2))
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
source('~/GitHub/Feature-Engineering-Project/R Scripts/Remove outliers from dataset.R')
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Select all numeric values
df5 <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, CMA))
# Remove Outliers
source('~/GitHub/Feature-Engineering-Project/R Scripts/Remove outliers from dataset.R')
outlierKD(df5, TMA)
index <- sample(1:nrow(df5),round(0.75*nrow(df5)))
train <- df5[index,]
test <- df5[-index,]
###################################
## Fit a linear regression model ##
###################################
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$TMA)^2)/nrow(test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(train)
f <- as.formula(paste("TMA ~", paste(n[!n %in% "TMA"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
## Predicting TMA using the neural network ##
#############################################
test1 <- subset(test, select = -c(TMA))
plot(nn)
test <- data.frame(test1, test$TMA)
names(test)[names(test) == 'test.TMA'] <- 'TMA'
pr.nn <- compute(nn,test[,1:20])
pr.nn_ <- pr.nn$net.result*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
test.r <- (test$TMA)*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
##########################
## Compare the two MSEs ##
##########################
print(paste(MSE.lm,MSE.nn))
##########################
## Plot the performance ##
##########################
par(mfrow=c(1,2))
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='blue', bty='n', cex=.95)
legend('bottomright',legend='NN',pch=18,col='red', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs predicted linear model',pch=18, cex=0.7)
par(mfrow=c(1,2))
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted Neural Network',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
plot(test$TMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
plot(test$TMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted Neural Network',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted Neural Network',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
plot(test$TMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Select all numeric values
df5 <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, CMA))
# Remove Outliers
source('~/GitHub/Feature-Engineering-Project/R Scripts/Remove outliers from dataset.R')
outlierKD(df5, TMA)
View(df5)
source('~/GitHub/Feature-Engineering-Project/R Scripts/2.R')
source('~/GitHub/Feature-Engineering-Project/R Scripts/3.R')
source('~/GitHub/Feature-Engineering-Project/R Scripts/2.R')
source('~/GitHub/Feature-Engineering-Project/R Scripts/3.R')
outlierKD1(df5, TMA)
source('~/GitHub/Feature-Engineering-Project/R Scripts/Remove outliers from dataset.R')
outlierKD1(df5, TMA)
outlierKD1(df5, TMA)
outlierKD2(df5, dataplus)
outlierKD3(df5, duelplane)
source('~/GitHub/Feature-Engineering-Project/R Scripts/3.R')
outlierKD3(df5, dualpane)
index <- sample(1:nrow(df5),round(0.75*nrow(df5)))
train <- df5[index,]
test <- df5[-index,]
###################################
## Fit a linear regression model ##
###################################
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$TMA)^2)/nrow(test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(train)
f <- as.formula(paste("TMA ~", paste(n[!n %in% "TMA"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
lm.fit <- glm(TMA~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$TMA)^2)/nrow(test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(train)
f <- as.formula(paste("TMA ~", paste(n[!n %in% "TMA"], collapse = " + ")))
nn <- neuralnet(f,data=train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
## Predicting TMA using the neural network ##
#############################################
test1 <- subset(test, select = -c(TMA))
test <- data.frame(test1, test$TMA)
names(test)[names(test) == 'test.TMA'] <- 'TMA'
pr.nn <- compute(nn,test[,1:20])
pr.nn_ <- pr.nn$net.result*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
test.r <- (test$TMA)*(max(df5$TMA)-min(df5$TMA))+min(df5$TMA)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test)
##########################
## Compare the two MSEs ##
##########################
print(paste(MSE.lm,MSE.nn))
##########################
## Plot the performance ##
##########################
par(mfrow=c(1,2))
plot(test$TMA,pr.nn_,col='red',main='Real vs predicted Neural Network',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
plot(test$TMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
points(test$TMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
plot(test$TMA,pr.lm,col='blue',main='Real vs Predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
# Load CSV
df4 <- read.csv("Normalized_fff_2014j_Dataset.csv")
# Look for missing values
apply(df4,2,function(x) sum(is.na(x)))
df4 <- na.omit(df4)
df5_CMA <- subset(df4, select = -c(df1.final_result, df1.disability, df1.age_band, df1.imd_band, df1.highest_education, df1.region, df1.gender, TMA))
# Split data into CMA_Training and CMA_Test set at 3/4ths split
index <- sample(1:nrow(df5_CMA),round(0.75*nrow(df5_CMA)))
CMA_Train <- df5_CMA[index,]
CMA_Test <- df5_CMA[-index,]
###################################
## Fit a linear regression model ##
###################################
lm.fit <- glm(CMA~., data=CMA_Train)
summary(lm.fit)
pr.lm <- predict(lm.fit,CMA_Test)
MSE.lm <- sum((pr.lm - CMA_Test$CMA)^2)/nrow(CMA_Test)
####################
## Fit Neural Net ##
####################
library(neuralnet)
n <- names(CMA_Train)
f <- as.formula(paste("CMA ~", paste(n[!n %in% "CMA"], collapse = " + ")))
nn <- neuralnet(f,data=CMA_Train,hidden=c(8,8), err.fct="sse",linear.output=TRUE, learningrate = 0.3, threshold = 0.5)
#############################################
## Predicting CMA using the neural network ##
#############################################
CMA_Test1 <- subset(CMA_Test, select = -c(CMA))
CMA_Test <- data.frame(CMA_Test1, CMA_Test$CMA)
names(CMA_Test)[names(CMA_Test) == 'CMA_Test.CMA'] <- 'CMA'
pr.nn <- compute(nn,CMA_Test[,1:19])
pr.nn_ <- pr.nn$net.result*(max(df5_CMA$CMA)-min(df5_CMA$CMA))+min(df5_CMA$CMA)
CMA_Test.r <- (CMA_Test$CMA)*(max(df5_CMA$CMA)-min(df5_CMA$CMA))+min(df5_CMA$CMA)
MSE.nn <- sum((CMA_Test.r - pr.nn_)^2)/nrow(CMA_Test)
pr.nn <- compute(nn,CMA_Test[,1:19])
pr.nn <- compute(nn,CMA_Test[,1:20])
pr.nn_ <- pr.nn$net.result*(max(df5_CMA$CMA)-min(df5_CMA$CMA))+min(df5_CMA$CMA)
CMA_Test.r <- (CMA_Test$CMA)*(max(df5_CMA$CMA)-min(df5_CMA$CMA))+min(df5_CMA$CMA)
MSE.nn <- sum((CMA_Test.r - pr.nn_)^2)/nrow(CMA_Test)
##########################
## Compare the two MSEs ##
##########################
print(paste(MSE.lm,MSE.nn))
##########################
## Plot the performance ##
##########################
par(mfrow=c(1,2))
plot(CMA_Test$CMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(CMA_Test$CMA,pr.lm,col='blue',main='Real vs predicted Linear Model',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
################
## More Plots ##
################
par(mfrow=c(1,1))
{plot(CMA_Test$CMA,pr.nn_,col='red',main='Real vs Predicted Neural Network',pch=18,cex=0.7)
points(CMA_Test$CMA,pr.lm,col='blue',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))}
plot(nn)
